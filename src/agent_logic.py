import os
import sys
from dotenv import load_dotenv

# Chargement des variables d'environnement (r√©cup√®re la cl√© depuis .env)
load_dotenv()

# V√©rification de s√©curit√©
if not os.getenv("MISTRAL_API_KEY"):
    print("‚ùå ERREUR : La cl√© MISTRAL_API_KEY est introuvable.")
    print("Assure-toi d'avoir cr√©√© le fichier .env √† la racine du projet.")
    sys.exit(1)

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_mistralai import ChatMistralAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- CONFIGURATION DES CHEMINS ---
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
VECTORSTORE_PATH = os.path.join(root_dir, "data", "vectorstore")

# --- 1. CHARGEMENT DE LA M√âMOIRE (RAG) ---
print("Chargement de la base vectorielle...")
embedding_function = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_db = Chroma(persist_directory=VECTORSTORE_PATH, embedding_function=embedding_function)

# RETRIEVER : C'est ici qu'on r√®gle la sensibilit√© !
# k=4: On r√©cup√®re les 4 morceaux les plus proches pour donner un max de contexte au LLM
# Cela compense le fait que la "Javel" puisse arriver en 4√®me.
retriever = vector_db.as_retriever(search_kwargs={"k": 4})

# --- 2. INITIALISATION DU CERVEAU (LLM) ---
# 'mistral-large-latest' est le plus intelligent. 
# Si tu veux √©conomiser, utilise 'open-mistral-nemo' ou 'mistral-small-latest'.
llm = ChatMistralAI(
    model="mistral-small-latest", 
    temperature=0.1  # Faible temp√©rature = R√©ponse factuelle et pr√©cise
)

# --- 3. D√âFINITION DE LA PERSONNALIT√â (Prompt) ---
# [cite_start]On utilise les sources [cite: 7, 50] pour d√©finir un agent RAG strict.
template = """
Tu es Eco-Sorter, un assistant expert en gestion des d√©chets pour la r√©gion de Bruxelles.
Ta mission est d'aider les citoyens √† trier correctement pour soutenir l'objectif de d√©veloppement durable.

CONSIGNES STRICTES :
1. Utilise UNIQUEMENT le contexte fourni ci-dessous pour r√©pondre.
2. Si la r√©ponse se trouve dans le contexte, sois pr√©cis : dis exactement dans quel sac (Jaune, Bleu, Blanc, Orange, Vert) ou quel lieu (Proxy Chimik, Recypark, Bulles √† verre) l'objet doit aller.
3. Si le contexte mentionne que c'est "INTERDIT" dans un sac, cherche dans le reste du contexte o√π c'est "AUTORIS√â".
4. Si tu ne trouves PAS la r√©ponse dans le contexte, dis poliment : "Je n'ai pas l'information pr√©cise dans mon guide pour cet objet. Par pr√©caution, v√©rifiez sur le site de Bruxelles-Propret√©." (N'invente rien).

CONTEXTE ISSU DU GUIDE DE TRI :
{context}

QUESTION DE L'UTILISATEUR : 
{question}

R√âPONSE :
"""
prompt = ChatPromptTemplate.from_template(template)

# --- 4. CR√âATION DE LA CHA√éNE (Pipeline) ---
def format_docs(docs):
    # Fonction pour "coller" les morceaux de texte ensemble
    return "\n\n".join([d.page_content for d in docs])

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# --- FONCTION D'INTERACTION ---
def ask_agent(user_input):
    print(f"\nüë§ Utilisateur : {user_input}")
    print("‚è≥ Eco-Sorter r√©fl√©chit...")
    try:
        response = rag_chain.invoke(user_input)
        print(f"ü§ñ Eco-Sorter : {response}")
        return response
    except Exception as e:
        print(f"‚ùå Erreur technique : {e}")
        return "D√©sol√©, une erreur est survenue."

if __name__ == "__main__":
    # --- ZONE DE TEST ---
    # Test 1 : Facile
    ask_agent("O√π je mets mes √©pluchures d'orange ?")
    
    # Test 2 : Le pi√®ge s√©mantique (Javel)
    ask_agent("J'ai un vieux bidon d' eau de Javel vide, poubelle bleue ?")
    
    # Test 3 : Le pi√®ge de la n√©gation (Plastique interdit)
    ask_agent("O√π je jette un seau en plastique cass√© ?")